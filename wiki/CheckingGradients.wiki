#summary How to verify that the gradients computed by the net are correct.

= Testing the gradients =

This code is capable of numerically testing the gradients that it computes for correctness. You can only do this on small models, because numerically computing a gradient for each weight is highly computationally-intensive.

But here's how to do it. You should read the [LayerParams] and [TrainingNet] sections first so you're familiar with how to run the net.

*Note also that in order for the numerical gradient testing computation to produce accurate results, you will need to use bigger weight initializations than you normally would (that's the initW parameter in layers that have weights).*

Once you've checked out the code, you can find some dummy architecture definition files in the `example-layers/` subdirectory. You can verify the gradients computed for that dummy architecture with this line:

{{{
python convnet.py --layer-def=./example-layers/layers.gc.cfg --layer-params=./example-layers/layer-params.gc.cfg --data-provider=dummy-cn-192 --check-grads=1
}}}

Notice that we're using a dummy data provider to produce 192-dimensional input, which is interpreted by the net as 8x8 3-channel images (see the *conv32* layer in layer definition file).

You'll see output that, in my case, looks like this:

{{{
Initialized data layer 'data', producing 192 outputs
Initialized data layer 'labels', producing 1 outputs
Initialized convolutional layer 'conv32', producing 6x6 16-channel output
Initialized convolutional layer 'conv32-2', producing 3x3 16-channel output
Initialized fully-connected layer 'fc10', producing 10 outputs
Initialized softmax layer 'probs', producing 10 outputs
Initialized logistic regression cost 'logprob'
=========================
Importing _ConvNet C++ module
=========================
Training ConvNet
Check gradients and quit?   : 1     
Compress checkpoints?       : 0     [DEFAULT]
Data batch range: testing   : 0-0   
Data batch range: training  : 0-0   
Data path                   :       
Data provider               : dummy-cn-192 
GPU override                :       [DEFAULT]
Layer definition file       : ./layers.gc.cfg 
Layer parameter file        : ./layer-params.gc.cfg 
Load file                   :       [DEFAULT]
Maximum save file size (MB) : 0     [DEFAULT]
Minibatch size              : 128   [DEFAULT]
Number of GPUs              : 1     [DEFAULT]
Number of epochs            : 50000 [DEFAULT]
Save path                   :       
Test and quit?              : 0     [DEFAULT]
Test on one batch at a time?: 1     [DEFAULT]
Testing frequency           : 50    [DEFAULT]
=========================
Running on CUDA device(s) -2
Current time: Thu Jun 30 20:38:02 2011
Saving checkpoints to ConvNet__Thu_Jun_30_20_38_02_2011
=========================
2.1...------------------------
ALL 6 TESTS PASSED
}}}

The model verifies the gradients of each of the weight matrices in the net. In this case there are 6 of them. Had any of the gradient tests failed, the model would have printed out the relative error between the results of the analytic computation and the numerical one. The relative error between two vectors is
{{{
||x_1 - x_2|| / ||x_1||
}}}

In the file [http://code.google.com/p/cuda-convnet/source/browse/trunk/include/util.cuh util.cuh], there are these two lines:
{{{
#define GC_SUPPRESS_PASSES     true
#define GC_REL_ERR_THRESH      0.02
}}}

The first one controls whether anything should be printed for gradients that pass. The second defines what a pass is. You can see that at present, any relative error smaller than 0.02 (2%) is considered a pass.

But please note that in general, numerical gradient testing is a bit of a black art. Nonlinearities in the network, especially in deep networks, make it difficult to accurately estimate the gradient numerically. A relative error greater than 2% does not _necessarily_ mean that the analytic gradient was computed incorrectly. In fact a mere 2% mismatch is a _very_ good result.

Each layer type in [http://code.google.com/p/cuda-convnet/source/browse/trunk/src/layer.cu layer.cu] defines a `checkGradients` method, in which it specifies the `eps` that should be used in the equation 
<pre>
f'(x) ~= (f(x + eps) - f(x)) / eps
</pre>
which is used to numerically compute the gradients. My experience has been that if your gradients are computed correctly, it's possible to find a value of `eps` to make the numerical computation match the analytic one. If your gradients are computed incorrectly, this is impossible.