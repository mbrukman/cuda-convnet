#summary How to specify a neural net architecture.

<h1>Table of Contents</h1>
<wiki:toc max_depth="4" />

= Introduction =

To define the architecture of your neural net, you must write a layer definition file. Here I will go through the format of the file and the parameters that the different layers take.


= File format =

== Layer definition file ==

I'll do this by example. In this example I'll cover all the layer types that this net supports.

===Data layer===

Let's make a new file called [http://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-example.cfg layers-example.cfg], which will define the architecture of our neural net. The first thing we'll add to it is a data layer:

{{{
[data]
type=data
dataIdx=0

[labels]
type=data
dataIdx=1

# By the way, this is a comment.
}}}

The things in square brackets are user-friendly layer names, and they can be whatever you want them to be. Here we're really defining two layers: one we're calling *data* and the other we're calling *labels*. 

The *type=data* line indicates that this is a data layer.

Our python [Data data provider] outputs a list of two elements: the CIFAR-10 images and the CIFAR-10 labels. The line *dataIdx=0* indicates that the layer named *data* is mapped to the CIFAR-10 images, and similarly the line *dataIdx=1* indicates that the layer named *labels* is mapped to the CIFAR-10 labels.

===Convolution layer===

Now let's apply a convolution to this data.

{{{
[conv32]
type=conv
inputs=data
channels=3
filters=32
padding=4
stride=1
filterSize=9
neuron=logistic
initW=0.00001
partialSum=1
sharedBiases=true
}}}

Again, the bracketed *conv32* is the name we're choosing to give to this layer.

Here's what the other parameters mean:
|| *Parameter* || *Default value* || *Meaning* ||
||`type=conv`||`--`||defines this as a convolution layer||
||`inputs=data` ||`--`||says that this layer will take as input the layer named *data* (defined above) ||
||`channels=3` ||`--`|| tells the net that the layer named *data* produces 3-channel images (i.e. color images). Since the images are assumed to be square, that is all that you have to tell it about the data dimensionality. This value must be either 1, 2, 3, or a multiple of 4.||
|| `filters=32`||`--`|| says that this layer will apply 32 filters to the images. This number must be a multiple of 16. ||
||`padding=4` ||`0`|| instructs the net to implicitly pad the images with a 4-pixel border of zeros (this does not cause it to create a copy of the data or use any extra memory). Set to 0 if you don't want padding.||
||`stride=1` ||`1`||indicates that the distance between successive filter applications should be 1 pixel. ||
||`filterSize=9` ||`--`||says that this layer will use filters of size 9x9 pixels (with 3 channels). ||
||`neuron=logistic` ||`--`|| defines the neuron non-linearity. See NeuronTypes for supported types.||
||`initW=0.00001` ||`--`|| instructs the net to initialize the weights in this layer from a normal distribution with mean zero and standard deviation 0.00001. The biases are always initialized at zero.||
||`sharedBiases=true` ||`true`|| indicates that the biases of every filter in this layer should be shared amongst all applications of that filter (which is how convnets are usually trained). Setting this to false will untie the biases, yielding a separate bias for every location at which the filter is applied. ||
||`partialSum=1` ||`--`|| this is a parameter that affects the performance of the weight gradient computation. It's a bit hard to predict what value will result in the best performance (it's problem-specific), but it's worth trying a few. Valid values are ones that divide the total number of outputs in this convolutional layer. This parameter also has an impact on memory consumption. The amount of extra memory used will be: `(number of weights in this layer)x(number of outputs this layer produces) / partialSum.` My belief is that since one of the defining features of convolutional nets is their small number of weights, this shouldn't be a big deal. ||

This convolutional layer will produce 32 x _N_ x _N_ outputs, where _N_ is the number of filter applications required to cover the entire width (equivalently, height) of the image plus zero-padding with the given stride. The filters will sum over all 3 input channels. 

The parameters with default values may be omitted from the configuration file.

====Performance notes====
  * The computation will be most efficient when `filters` is divisible by 32.

===Locally-connected layer with unshared weights===

This kind of layer is just like a convolutional layer without any weight-sharing. That is to say, a different set of filters is applied at every location in the image. Aside from that, it behaves exactly as a convolutional layer.

Here's how to define such a layer, taking the *conv32* layer as input:

{{{
[local32]
type=local
inputs=conv32
channels=32
filters=32
padding=4
stride=1
filterSize=9
neuron=logistic
initW=0.00001
}}}

Aside from the *type=local* line, there's nothing new here.

====Performance notes====
  * The computation will be most efficient when `filters` is divisible by 32.

===Fully-connected layer===

Now we'll add a fully-connected layer over the data too:
{{{
[fc1024]
type=fc
outputs=1024
inputs=data
initW=0.001
neuron=relu
}}}

The only parameter here that we have not yet seen is *outputs=1024*. It does what you would expect -- it indicates that this layer should have 1024 neurons.

===Local pooling layer===

Let's add a local max-pooling layer over our locally-connected layer.

{{{
[maxpool]
type=pool
pool=max
inputs=local32
start=0
sizeX=4
stride=2
outputsX=0
channels=32
}}}

A few new parameters here:
|| *Parameter* || *Default value* || *Meaning* ||
||`pool=max` ||`--`||indicates that this is to be a max-pooling layer. Also supported is *pool=avg* for average-pooling. ||
||`inputs=conv32` ||`--`||indicates that this layer subsamples the layer named *conv32*.||
||`start=0` ||`0`||tells the net where in the image to start the pooling. In principle, you can start anywhere you want. Setting this to a positive number will cause the net to discard some pixels at the top and at the left of the image. Setting this to a negative number will cause it to include pixels that don't exist (which is fine). *start=0* is the usual setting. ||
||`sizeX=4` ||`--`||defines the size of the pooling region in the x (equivalently, y) dimension. Squares of size (*sizeX*)^2^ get reduced to one value by this layer. There are no restrictions on the value of this parameter. It's fine for a pooling square to fall off the boundary of the image.  ||
||`stride=2` ||`--`||defines the stride size between successive pooling squares. Setting this parameter smaller than *sizeX* produces _overlapping_ pools. Setting it equal to *sizeX* gives the usual, non-overlapping pools. Values greater than *sizeX* are not allowed. ||
||`outputsX=0` ||`0`||allows you to control how many outputs in the x (equivalently, y) dimension this operation will produce. This parameter is analogous to the *start* parameter, in that it allows you to discard some portion of the image by setting it to a value small enough to leave part of the image uncovered. Setting it to zero instructs the net to produce as many outputs as is necessary to ensure that the whole image is covered. ||

===Local response normalization layer===

Now let's add a local response normalization layer over the pooling layer. This layer will compute the function

<wiki:gadget url="http://mathml-gadget.googlecode.com/svn/trunk/mathml-gadget.xml" border="0" up_content="f(x_i) = x_i / (1 + alpha/N^2 sum_j x_j^2)^beta" height="75"/>

for _x,,j,,_ in a local neighborhood of _x,,i,,_ of size _N_ x _N_. The output dimensionality of this layer is of course always equal to the input dimensionality. 

This type of layer turns out to be useful when using neurons with unbounded activations (e.g. rectified linear neurons), because it permits the detection of high-frequency features with a big neuron response, while damping responses that are uniformly large in a local neighborhood. It is a type of regularizer that encourages "competition" for big activities among nearby groups of neurons. 

Here's how this layer is specified:

{{{
[rnorm1]
type=rnorm
inputs=maxpool
channels=32
sizeX=5
scale=0.0000125
pow=0.75
}}}

|| *Parameter* || *Meaning* ||
|| `channels=32` || indicates that this layer takes 32-channel input because that's what the *maxpool* layer produces. The number of "channels" here just serves to define the shape of the input and has no actual bearing on the output (unlike in convolutional layers, which sum over channels). ||
|| `sizeX=11` || the _N_ variable in the above formula, this defines the size of the local regions used for response normalization. Squares of (*sizeX*)^2^ are used to normalize each pixel. The squares are centered at the pixel. ||
|| `scale=0.0000125` || the _alpha_ variable in the above formula, this weights the response normalization term. Setting this to zero makes this layer's output equal to its input -- i.e., you get no response normalization.||
|| `pow=0.75` || the _beta_ variable in the above formula. Setting this to zero likewise makes this layer's output equal to its input. ||

===Local contrast normalization layer===

Now let's add a local *contrast* normalization layer over the response normalization layer. This layer will compute the function

<wiki:gadget url="http://mathml-gadget.googlecode.com/svn/trunk/mathml-gadget.xml" border="0" up_content="f(x_i) = x_i / (1 + alpha/N^2 sum_j (x_j-m_i)^2)^beta" height="75"/>

for _x,,j,,_ in a local neighborhood of _x,,i,,_ of size _N_ x _N_. _m,,i,,_ here is the mean of all _x,,j,,_ in the _N_ x _N_ neighborhood around _x,,i,,_. This layer is very similar computationally to response normalization -- the difference being that the denominator here computes the variance of activities in each neighborhood, rather than just the sum of squares.

Here's how this layer is specified:

{{{
[cnorm1]
type=cnorm
inputs=rnorm1
channels=32
sizeX=7
scale=0.001
pow=0.5
}}}

The meanings of all of these parameters are the same as in the response normalization layer described above.

=== Block sparse convolution layer ===

Now let's define a convolutional layer in which each filter only looks at _some_ of the input channels.

{{{
[conv32-2]
type=conv
inputs=cnorm1
groups=4
channels=32
filters=32
padding=2
stride=1
filterSize=5
neuron=relu
initW=0.0001
partialSum=1
sharedBiases=false
}}}

The primary novelty here is the *groups=4* parameter. Together with the *filters=32* parameter, they state that this convolutional layer is to have 4 groups of 32 filters. Each filter will connect to (i.e. sum over) 8 input channels (a quarter of the total number of input channels).

The following diagram depicts the kind of operation that this layer will perform (note however that the diagram depicts two filters per group rather than 32).

http://cuda-convnet.googlecode.com/svn/wiki/images/conv-diagram.png

*Block sparsity may be used in convolutional layers as well as in locally-connected, unshared layers.*

====Performance notes====
  * Block sparse convolutional layers are just as efficient computationally as non-sparse convolutional layers.
  * The computation will be most efficient when `channels / groups` is divisible by 16 and `filters` is divisible by 32.

=== Random sparse convolution layer ===

A random sparse convolution layer is similar to the block sparse convolution layer described above, but here each group of filters connects to a _random_ subset of channels in the layer below.

Here's how this kind of layer is specified:

{{{
[conv32-3]
type=conv
inputs=conv32-2
groups=4
channels=128
filters=32
padding=2
stride=2
filterSize=5
neuron=relu
initW=0.0001
partialSum=1
randSparse=true
filterChannels=64
}}}

Notice that this layer has 128-channel input, because the layer just above has 128 (4 groups of 32) filters. This layer will also produce 128-channel output.

There are two new parameters here:

|| *Parameter* || *Default value* || *Meaning* ||
||`randSparse=true` ||`false`|| indicates that random sparsity should be used in this layer. When this parameter is true, the `groups` parameter must be greater than 1 (otherwise the connectivity will not be sparse). ||
||`filterChannels=64` ||`channels/groups`||an optional parameter indicating how many input channels each filter should look at. Here we've specified 64, which means that each group will be connected to 64 randomly-chosen input channels, and each input channel will be connected to exactly two groups. Omitting this parameter will cause each input channel to be connected to exactly one (randomly-chosen) group.||

*Random sparsity may be used in convolutional layers as well as in locally-connected, unshared layers.*

====Performance notes====
  * Random sparse convolutional layers are only about 1% less efficient computationally than block sparse convolutional layers, which themselves are as efficient as non-sparse convolutional layers. So there's no performance reason to avoid them. 
  * The computation will be most efficient when `filterChannels` is divisible by 16 and `filters` is divisible by 32.

===Fully-connected layer with multiple inputs; softmax layer===

Now let's connect this convolutional layer and the fully-connected layer defined long ago to a softmax output:
{{{
[fc10]
type=fc
outputs=10
inputs=conv32-2,fc1024
initW=0.0001,0.0001
neuron=ident

[probs]
type=softmax
inputs=fc10
}}}

The main point of this example is to illustrate that a fully-connected layer can take multiple inputs. Here *fc10* is taking input from *conv32-2* and *fc1024*, and produces 10 outputs. Notice that because *fc10* takes two inputs, it has two weight matrices, and therefore the *initW* parameter must be a comma-delimited list of floats. Their order corresponds to the order given to the *inputs* parameter.

The *probs* definition is a softmax layer that, as you might expect, applies a softmax to the 10 outputs produced by the *fc10* layer. The reason that *fc10* produces 10 outputs is that there are 10 classes in the CIFAR-10 dataset, so *it is important to match the size of this layer to the number of classes in your dataset*.

===Logistic regression cost layer===

Finally, let's define an objective function for the net to optimize. You can define and simultaneously optimize multiple objectives, but we'll stick to one here. The objective is the (multinomial) [http://en.wikipedia.org/wiki/Multinomial_logistic_regression logistic regression] objective.

{{{
[logprob]
type=cost.logreg
inputs=labels,probs
}}}

The *cost.logreg* objective takes two inputs -- labels and predicted probabilities. We defined the *labels* layer early on, and the *probs* layer just above.

That's it! The convolutional net's architecture is defined.

== Layer parameter file ==

There's one problem though -- we haven't specified any learning parameters (learning rates, etc.). This is done via another file, which we'll call [http://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layer-params-example.cfg layer-params-example.cfg]. The idea is that you'll use one file to define the net's architecture, which stays fixed for the duration of training, and another file to define learning parameters, which you can change while the net is training.

So let's do that. I'll now go through the contents of the [http://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layer-params-example.cfg layer-params-example.cfg] file that corresponds to the net defined above in [http://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-example.cfg layers-example.cfg].

Not all layer types require learning parameters. Layers without weights (for example pooling layers) have none. The layers mentioned below are the only ones that require learning parameters.

===Convolutional layer===

{{{
[conv32]
epsW=0.001
epsB=0.002
momW=0.9
momB=0.9
wc=0
}}}

This specifies the learning parameters for the layer *conv32*. Here's what the parameters mean:
|| *Parameter* || *Meaning* ||
||epsW=0.001 ||the weight learning rate. ||
||epsB=0.002 ||the bias learning rate. ||
||momW=0.9||the weight momentum. ||
||momB=0.9||the bias momentum. ||
||wc=0 || the L2 weight decay (applied to the weights but not the biases).||

Given these values, the update rule for the weights is:
{{{
weight_inc[i] := momW * weight_inc[i-1] - wc * epsW * weights[i-1] + epsW * weight_grads[i]
weights[i] := weights[i-1] + weight_inc[i]
}}}

where `weight_grads[i]` is the average gradient over minibatch `i`. The update rule for biases is the same except that there is no bias weight decay.

Now that that's out of the way, there isn't much left to describe.

===Locally-connected layer with unshared weights==

This type of layer takes the same types of parameters as a convolutional layer.
{{{
[local32]
epsW=0.001
epsB=0.002
momW=0.9
momB=0.9
wc=0
}}}

===Fully-connected layer===

We add learning parameter definitions for the *fc1024* layer too:
{{{
[fc1024]
momW=0.9
momB=0.9
epsW=0.00001
epsB=0.00002
wc=0
}}}

And the *conv32-2* and *conv32-3* layers:

{{{
[conv32-2]
epsW=0.001
epsB=0.002
momW=0.9
momB=0.9
wc=0

[conv32-3]
epsW=0.001
epsB=0.002
momW=0.9
momB=0.9
wc=0
}}}

The *fc10* layer:
{{{
[fc10]
epsW=0.0001,0.001
epsB=0.002
momW=0.5,0.9
momB=0.9
wc=0,0
}}}

Here there is one twist: since the *fc10* layer takes two inputs, it has two weight matrices. So just like with the *initW* parameter, we have to provide *fc10* with a list of learning rates, momenta, and weight decays. We do this by writing them as a comma-delimited list of values.

===Logistic regression cost layer===

Finally, the *logprob* layer takes one parameter:
{{{
[logprob]
coeff=1
}}}

... a scalar coefficient of the objective function. This provides an easy way to tweak the "global" learning rate of the network. Note, however, that tweaking this parameter is not equivalent to tweaking the *epsW* parameter of every layer in the net if you use weight decay. That's because tweaking *coeff* will leave the effective weight decay coefficient (`epsW*wc`) unchanged. But tweaking *epsW* will of course cause a change in `epsW*wc`.

= Training the net =

Now the net is fully-defined and ready to train. See TrainingNet for details about how to actually cause training to happen.