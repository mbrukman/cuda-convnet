#summary How to specify a neural net architecture.

= Introduction =

To define the architecture of your neural net, you must write a layer definition file. Here I will go through the format of the file and the parameters that the different layers take.


= File format =

== Layer definition file ==

I'll do this by example. In this example I'll cover all the layer types that this code supports.

Let's make a new file called `layers.cfg`, which will define the architecture of our neural net. The very first thing we'll add to it is a data layer:

{{{
[data]
type=data
dataIdx=0

[labels]
type=data
dataIdx=1

# By the way, this is a comment.
}}}

The things in square brackets are user-friendly layer names, and they can be whatever you want them to be. Here we're really defining two layers: one we're calling *data* and the other we're calling *labels*. 

The *type=data* line indicates that this is a data layer. Our python [Data data provider] outputs a list of two elements: the CIFAR-10 images and the CIFAR-10 labels. 

The line *dataIdx=0* indicates that the layer named *data* is mapped to the CIFAR-10 images (and the layer named *labels* is mapped to the CIFAR-10 labels).

Now let's apply a convolution to this data.

{{{
[conv32]
type=conv
inputs=data
channels=3
numFilters=32
padding=4
stride=1
filterSize=9
neuron=logistic
initW=0.00001
partialSum=1
sharedBiases=true
}}}

Again, the bracketed *conv32* is the name we're choosing to give to this layer.

Here's what the other parameters mean:
|| *Parameter* || *Meaning* ||
||type=conv||defines this as a convolution layer||
||inputs=data ||says that this layer will take as input the layer named *data* (defined above) ||
||channels=3 || tells the net that the layer named *data* produces 3-channel images (i.e. color images). Since the images are assumed to be square, that is all that you have to tell it about the data dimensionality. This value must be either 1, 2, 3, or a multiple of 4.||
|| numFilters=32|| says that this layer will apply 32 filters to the images. This number must be a multiple of 16. *The convolution will be most efficient if it is a multiple of 32.*||
||padding=4 || instructs the net to implicitly pad the images with a 4-pixel border of zeros (this does not cause it to create a copy of the data or use any extra memory). Set to 0 if you don't want padding.||
||stride=1 ||indicates that the distance between successive filter applications should be 1 pixel. ||
||filterSize=9 ||says that this layer will use filters of size 9x9 pixels (with 3 channels). ||
||neuron=logistic || defines the neuron nonlinearity. See NeuronTypes for supported types.||
||initW=0.00001 || instructs the net to initialize the weights in this layer from a normal distribution with mean zero and standard deviation 0.00001. The biases are always initialized at zero.||
||partialSum=1 || this is a parameter that affects the performance of the weight gradient computation. It's a bit hard to predict what value will result in the best performance (it's problem-specific), but it's worth trying a few. Valid values are ones that divide the total number of outputs in this convolutional layer. This parameter also has an impact on memory consumption. The amount of extra memory used will be: `(number of weights in this layer)x(number of outputs this layer produces) / partialSum.` Since one of the defining features of convolutional nets is their small number of weights, this shouldn't be a big deal. ||
||sharedBiases=true || indicates that the biases of every filter in this layer should be shared amongst all applications of that filter (which is how convnets are usually trained). Setting this to false will untie the biases, so there will be a separate bias for every location at which the filter is applied. ||

For kicks, let's add a fully-connected layer over the data too:
{{{
[fc1024]
type=fc
numOutputs=1024
inputs=data
initW=0.001
neuron=relu
}}}

The only parameter here that we have not yet seen is *numOutputs=1024*. It does what you would expect -- it indicates that this layer should have 1024 units.

Back to convolution now, let's add a local max-pooling layer over our convolution layer.

{{{
[maxpool]
type=pool
pool=max
inputs=conv32
start=0
sizeX=4
stride=2
outputsX=0
channels=32
}}}

A few new parameters here:
|| *Parameter* || *Meaning* ||
||pool=max ||indicates that this is to be a max-pooling layer. Also supported is *pool=avg* for average-pooling. ||
||inputs=conv32 ||indicates that this layer subsamples the layer named *conv32*.||
||start=0 ||tells the net where in the image to start the pooling. In principle, you can start anywhere you want. Setting this to a positive number will cause the net to discard some pixels at the top and at the left of the image. Setting this to a negative number will cause it to include pixels that don't exist (which is fine). *start=0* is the usual setting. ||
||sizeX=4 ||defines the size of the pooling region in the x (equivalently, y) dimension. Squares of size (*sizeX*)^2^ get reduced to 1 value by this layer. There are no restrictions on the value of this parameter. It's fine for a pooling square to fall off the boundary of the image.  ||
||stride=2 ||defines the stride size between successive pooling squares. Setting this parameter smaller than *sizeX* produces _overlapping_ pools. Setting it equal to *sizeX* gives the usual, non-overlapping pools. Values greater than *sizeX* are not allowed. ||
||outputsX=0 ||allows you to control how many outputs in the x (equivalently, y) dimension this operation will produce. This parameter is analogous to the *start* parameter, in that it allows you to discard some portion of the image by setting it to a value small enough to leave part of the image uncovered. Setting it to 0 instructs the net to produce as many outputs as is necessary to ensure that the whole image is covered. ||

Now let's add a local contrast normalization layer over the pooling layer. This layer will compute the function

<wiki:gadget url="http://mathml-gadget.googlecode.com/svn/trunk/mathml-gadget.xml" border="0" up_content="f(x_i) = x_i / (1 + \alpha/N^2 sum_j x_j^2)" height="75"/>

for _x,,j,,_ in a local neighbourhood of _x,,i,,_ of size _N_ x _N_. The output dimensionality of this layer is of course always equal to the input dimensionality. Here's how this layer is specified:

{{{
[cnorm1]
type=cnorm
inputs=maxpool
channels=32
sizeX=5
scale=0.0000125
}}}

|| *Parameter* || *Meaning* ||
|| channels=32 || indicates that this layer takes 32-channel input because that's what the *maxpool* layer produces. The number of "channels" here just serves to define the shape of the input and has no actual bearing on the output (unlike in convolutional layers, which sum over channels). ||
|| sizeX=11 || the _N_ variable in the above formula, this defines the size of the local regions used for contrast normalization. Squares of (*sizeX*)^2^ are used to normalize each pixel. The squares are centered at the pixel. ||
|| scale=0.0000125 || the _alpha_ variable in the above formula, this weights the contrast normalization term. Setting this to zero obviously makes this layer's output equal to its input -- i.e., you get no contrast normalization.||

OK, let's add one more convolutional layer over the contrast normalization layer:

{{{
[conv32-2]
type=conv
inputs=cnorm1
numFilters=32
padding=2
stride=1
filterSize=5
channels=32
neuron=relu
initW=0.0001
partialSum=1
sharedBiases=false
}}}

Nothing new here. Notice that this convolutional layer has 32 channels, because the *conv32* layer (and hence the *maxpool* layer, and hence the *cnorm1* layer) produces images with 32 channels. Therefore each filter in this layer will have 5x5x32 weights.

Now let's connect this convolutional layer and the fully-connected layer defined long ago to a softmax output:
{{{
[fc10]
type=fc
numOutputs=10
inputs=conv32-2,fc1024
initW=0.001,0.001
neuron=ident

[probs]
type=softmax
inputs=fc10
}}}

The main point of this example is to illustrate that a fully-connected layer can take multiple inputs. Here *fc10* is taking input from *conv32-2* and *fc1024*, and produces 10 outputs. Notice that because *fc10* takes two inputs, it has two weight matrices, and therefore the *initW* parameter must be a comma-delimited list of floats. Their order corresponds to the order given to the *inputs* parameter.

The *probs* definition is a softmax layer that, as you might expect, applies a softmax to the 10 outputs produced by the *fc10* layer.

Finally, let's define an objective function for the net to optimize. You can define and simultaneously optimize multiple objectives, but we'll stick to one here.

{{{
[logprob]
type=cost.logreg
inputs=labels,probs
}}}

The *cost.logreg* objective takes two inputs -- labels and predicted probabilities. We defined the *labels* layer early on, and the *probs* layer just above.

That's it! The convolutional net's architecture is defined.

== Layer parameter file ==

There's one problem though -- we haven't specified any learning parameters. This is done via another file, which we'll call `layer-params.cfg`. The idea is that you'll use one file to define the net's architecture, which stays fixed for the duration of training, and another file to define learning parameters, which can change while the net is training.

So let's do that. I'll now go through the contents of the `layer-params.cfg` file that corresponds to the net defined above in `layers.cfg`.

{{{
[conv32]
epsW=0.001
epsB=0.002
momW=0.9
momB=0.9
wc=0
}}}

This specifies the learning parameters for the layer *conv32* (defined early on). Here's what the parameters mean:
|| *Parameter* || *Meaning* ||
||epsW=0.001 ||the weight learning rate. ||
||epsB=0.002 ||the bias learning rate. ||
||momW=0.9||the weight momentum. ||
||momB=0.9||the bias momentum. ||
||wc=0 || the L2 weight decay (applied to the weights only).||

Given these values, the update rule for the weights is:
{{{
weight_inc[i] := momW * weight_inc[i-1] - wc * epsW * weights[i-1] + epsW * weight_grads[i]
weights[i] := weights[i-1] + weight_inc[i]
}}}

The update rule for biases is the same except that there is no bias weight decay.

Now that that's out of the way, there isn't much left to describe.

We add learning parameter definitions for the *fc1024* layer too:
{{{
[fc1024]
momW=0.9
momB=0.9
epsW=0.00001
epsB=0.00002
wc=0
}}}

And the *conv32-2* layer:

{{{
[conv32-2]
epsW=0.001
epsB=0.002
momW=0.9
momB=0.9
wc=0
}}}

The *fc10* layer:
{{{
[fc10]
epsW=0.0001,0.001
epsB=0.002
momW=0.5,0.9
momB=0.9
wc=0,0
}}}

Here there is one twist: since the *fc10* layer takes two inputs, it has two weight matrices. So just like with the *initW* parameter, we have to provide *fc10* with a list of learning rates, momenta, and weight decays. We do this by writing them as a comma-delimited list of values.

Finally, the *logprob* layer takes one parameter:
{{{
[logprob]
coeff=1
}}}

... a scalar coefficient of the objective function. This provides an easy way to tweak the "global" learning rate of the network.

= Training the net =

Now the net is fully-defined and ready to train. See TrainingNet for details about how to actually cause training to happen.