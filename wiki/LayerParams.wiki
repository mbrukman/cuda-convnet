#summary How to specify a neural net architecture.

<h1>Dec 9, 2011 note: this document is in the process of being updated</h1>

<h1>Table of Contents</h1>
<wiki:toc max_depth="4" />

= Introduction =

To define the architecture of your neural net, you must write a layer definition file. You can find several complete and working layer definition files [http://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/ here]. In this section I will go over the types of layers supported by the net, and how to specify them.

=Layer definition file - basic features =

==Data layer==

The first thing you might add to your layer definition file is a data layer.

{{{
[data]
type=data
dataIdx=0

[labels]
type=data
dataIdx=1

# By the way, this is a comment.
}}}

The things in square brackets are user-friendly layer names, and they can be whatever you want them to be. Here we're really defining two layers: one we're calling *data* and the other we're calling *labels*. 

The *type=data* line indicates that this is a data layer.

Our python [Data data provider] outputs a list of two elements: the CIFAR-10 images and the CIFAR-10 labels. The line *dataIdx=0* indicates that the layer named *data* is mapped to the CIFAR-10 images, and similarly the line *dataIdx=1* indicates that the layer named *labels* is mapped to the CIFAR-10 labels.

==Convolution layer==

Convolution layers apply a small set of filters all over their input "images". They're specified like this:

{{{
[conv32]
type=conv
inputs=data
channels=3
filters=32
padding=4
stride=1
filterSize=9
neuron=logistic
initW=0.00001
partialSum=1
sharedBiases=true
}}}

Again, the bracketed *conv32* is the name we're choosing to give to this layer.

Here's what the other parameters mean:
|| *Parameter* || *Default value* || *Meaning* ||
||`type=conv`||`--`||defines this as a convolution layer||
||`inputs=data` ||`--`||says that this layer will take as input the layer named *data* (defined above) ||
||`channels=3` ||`--`|| tells the net that the layer named *data* produces 3-channel images (i.e. color images). Since the images are assumed to be square, that is all that you have to tell it about the data dimensionality. This value must be either 1, 2, 3, or a multiple of 4.||
|| `filters=32`||`--`|| says that this layer will apply 32 filters to the images. This number must be a multiple of 16. ||
||`padding=4` ||`0`|| instructs the net to implicitly pad the images with a 4-pixel border of zeros (this does not cause it to create a copy of the data or use any extra memory). Set to 0 if you don't want padding.||
||`stride=1` ||`1`||indicates that the distance between successive filter applications should be 1 pixel. ||
||`filterSize=9` ||`--`||says that this layer will use filters of size 9x9 pixels (with 3 channels). ||
||`neuron=logistic` ||`ident`|| defines the neuron activation function to be applied to the output of the convolution. If omitted, no function is applied. See NeuronTypes for supported types. *All layers except data and cost layers can take a neuron parameter.*||
||`initW=0.00001` ||`--`|| instructs the net to initialize the weights in this layer from a normal distribution with mean zero and standard deviation 0.00001.||
||`initB=0` ||`0`|| instructs the net to initialize the biases in this layer to zero.||
||`sharedBiases=true` ||`true`|| indicates that the biases of every filter in this layer should be shared amongst all applications of that filter (which is how convnets are usually trained). Setting this to false will untie the biases, yielding a separate bias for every location at which the filter is applied. ||
||`partialSum=1` ||`--`|| this is a parameter that affects the performance of the weight gradient computation. It's a bit hard to predict what value will result in the best performance (it's problem-specific), but it's worth trying a few. Valid values are ones that divide the total number of outputs in this convolutional layer. This parameter also has an impact on memory consumption. The amount of extra memory used will be: `(number of weights in this layer)x(number of outputs this layer produces) / partialSum.` My belief is that since one of the defining features of convolutional nets is their small number of weights, this shouldn't be a big deal. ||

This convolutional layer will produce 32 x _N_ x _N_ outputs, where _N_ is the number of filter applications required to cover the entire width (equivalently, height) of the image plus zero-padding with the given stride. As mentioned above, each filter in this layer has 3x9x9 weights.

The parameters with default values may be omitted from the configuration file.

===Performance notes===
  * The computation will be most efficient when `filters` is divisible by 32.

==Locally-connected layer with unshared weights==

This kind of layer is just like a convolutional layer, but without any weight-sharing. That is to say, a different set of filters is applied at every (x, y) location in the input image. Aside from that, it behaves exactly as a convolutional layer.

Here's how to define such a layer, taking the *conv32* layer as input:

{{{
[local32]
type=local
inputs=conv32
channels=32
filters=32
padding=4
stride=1
filterSize=9
neuron=logistic
initW=0.00001
initB=0
}}}

Aside from the *type=local* line, there's nothing new here. Note however that since there is no weight sharing, the actual number of distinct filters in this layer will be `filters` multiplied by however many filter applications are required to cover the entire image and padding with the given stride.

===Performance notes===
  * The computation will be most efficient when `filters` is divisible by 32.

==Fully-connected layer==

A fully-connected layer simply multiplies its input by a weight matrix. It's specified like this:

{{{
[fc1024]
type=fc
outputs=1024
inputs=data
initW=0.001
neuron=relu
}}}

The only parameter here that we have not yet seen is *outputs=1024*. It does what you would expect -- it indicates that this layer should have 1024 neurons.

==Local pooling layer==

This type of layer performs local, *per-channel* pooling on its input.

{{{
[maxpool]
type=pool
pool=max
inputs=local32
start=0
sizeX=4
stride=2
outputsX=0
channels=32
neuron=relu
}}}

A few new parameters here:
|| *Parameter* || *Default value* || *Meaning* ||
||`pool=max` ||`--`||indicates that this is to be a max-pooling layer. Also supported is *pool=avg* for average-pooling. ||
||`inputs=conv32` ||`--`||indicates that this layer subsamples the layer named *conv32*.||
||`start=0` ||`0`||tells the net where in the input image to start the pooling (in x,y coordinates). In principle, you can start anywhere you want. Setting this to a positive number will cause the net to discard some pixels at the top and at the left of the image. Setting this to a negative number will cause it to include pixels that don't exist (which is fine). *start=0* is the usual setting. ||
||`sizeX=4` ||`--`||defines the size of the pooling region in the x (equivalently, y) dimension. Squares of size (*sizeX*)^2^ get reduced to one value by this layer. There are no restrictions on the value of this parameter. It's fine for a pooling square to fall off the boundary of the image.||
||`stride=2` ||`--`||defines the stride size between successive pooling squares. Setting this parameter smaller than *sizeX* produces _overlapping_ pools. Setting it equal to *sizeX* gives the usual, non-overlapping pools. Values greater than *sizeX* are not allowed. ||
||`outputsX=0` ||`0`||allows you to control how many outputs in the x (equivalently, y) dimension this operation will produce. This parameter is analogous to the *start* parameter, in that it allows you to discard some portion of the image by setting it to a value small enough to leave part of the image uncovered. Setting it to zero instructs the net to produce as many outputs as is necessary to ensure that the whole image is covered. ||

Since the pooling performed by this layer is per-channel, the number of output channels is equivalent to the number of input channels. But the size of the image in the x,y dimensions gets reduced.

==Local response normalization layer==

This kind of layer computes the function

http://cuda-convnet.googlecode.com/svn/wiki/images/rnorm.gif

for _x,,j,,_ in a local neighborhood of _x,,i,,_ of size _N_ x _N_. The output dimensionality of this layer is always equal to the input dimensionality. 

This type of layer turns out to be useful when using neurons with unbounded activations (e.g. rectified linear neurons), because it permits the detection of high-frequency features with a big neuron response, while damping responses that are uniformly large in a local neighborhood. It is a type of regularizer that encourages "competition" for big activities among nearby groups of neurons. 

Here's how this layer is specified:

{{{
[rnorm1]
type=rnorm
inputs=maxpool
channels=32
sizeX=5
scale=0.0000125
pow=0.75
}}}

|| *Parameter* || *Meaning* ||
|| `channels=32` || indicates that this layer takes 32-channel input because that's what the *maxpool* layer produces. The number of "channels" here just serves to define the shape of the input and has no actual bearing on the output (unlike in convolutional layers, which sum over channels). ||
|| `sizeX=11` || the _N_ variable in the above formula, this defines the size of the local regions used for response normalization. Squares of (*sizeX*)^2^ are used to normalize each pixel. The squares are centered at the pixel. ||
|| `scale=0.0000125` || the _alpha_ variable in the above formula, this weights the response normalization term. Setting this to zero makes this layer's output equal to its input -- i.e., you get no response normalization.||
|| `pow=0.75` || the _beta_ variable in the above formula. Setting this to zero likewise makes this layer's output equal to its input. ||

==Summation layer==

This kind of layer simply performs an element-wise summation of its input layers. Of course this implies that all of its input layers must have the same dimensionality. It's specified like this:

==Neuron layer==

This layer takes one layer as input and applies a neuron activation function to it. See NeuronTypes for available activation functions.

This layer is specified like this:

{{{
[neuron1]
type=neuron
inputs=layer1
neuron=logistic
}}}

Note that all layers except data layers and cost layers can take a *neuron=x* parameter, so there is often no need to explicitly define these neuron layers.

Side note: this layer is admittedly misnamed. It just applies an element-wise function to its input. So don't think of neurons with weights. This layer has no weights.

{{{
[sumlayer]
type=sum
inputs=layer1,layer2,layer3
}}}

This layer will produce the sum of the outputs of *layer1*, *layer2*, and *layer3*.

The number of outputs in this layer is of course equal to the number of outputs in any of the input layers.

==Local contrast normalization layer==

This kind of layer computes the function

http://cuda-convnet.googlecode.com/svn/wiki/images/cnorm.gif

for _x,,j,,_ in a local neighborhood of _x,,i,,_ of size _N_ x _N_. _m,,i,,_ here is the mean of all _x,,j,,_ in the _N_ x _N_ neighborhood around _x,,i,,_. This layer is very similar computationally to response normalization -- the difference being that the denominator here computes the variance of activities in each neighborhood, rather than just the sum of squares.

Here's how this layer is specified:

{{{
[cnorm1]
type=cnorm
inputs=rnorm1
channels=32
sizeX=7
scale=0.001
pow=0.5
}}}

The meanings of all of these parameters are the same as in the response normalization layer described above.

==Softmax layer==

This layer computes the function 

http://cuda-convnet.googlecode.com/svn/wiki/images/softmax.gif

where each _x,,i,,_ is an input. It's specified like this:

{{{
[fc10]
type=fc
outputs=10
inputs=conv32
initW=0.0001

[probs]
type=softmax
inputs=fc10
}}}

Here we're really defining two layers -- *fc10* is a fully-connected layer with 10 outputs (because our dataset has 10 classes), and *probs* is a softmax that takes *fc10* as input and produces 10 values which can be interpreted as probabilities. This type of layer is useful for classification tasks.

==Logistic regression cost layer==

A net must define an objective function to optimize. Here we're defining a (multinomial) [http://en.wikipedia.org/wiki/Multinomial_logistic_regression logistic regression] objective. It's specified like this:

{{{
[logprob]
type=cost.logreg
inputs=labels,probs
}}}

The *cost.logreg* objective takes two inputs -- true labels and predicted probabilities. We defined the *labels* layer early on, and the *probs* layer just above.

= Layer parameter file - basic features =

In this file we can specify learning parameters that we may want to change during the course of training (learning rates, etc.). You can find examples of this kind of file [http://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/ here].

The idea is that you'll use one file to define the net's architecture, which stays fixed for the duration of training, and another file to define learning parameters, which you can change while the net is training.

I'll now go through the learning parameters that the above-defined layers take. Generally, only layers with weights require learning parameters.

==Layers with weights==

Here's how to specify learning parameters for layers with weights (convolutional layers, locally-connected layers, and fully-connected layers).

{{{
[conv32]
epsW=0.001
epsB=0.002
momW=0.9
momB=0.9
wc=0
}}}

This specifies the learning parameters for the layer *conv32*. Here's what the parameters mean:
|| *Parameter* || *Meaning* ||
||epsW=0.001 ||the weight learning rate. ||
||epsB=0.002 ||the bias learning rate. ||
||momW=0.9||the weight momentum. ||
||momB=0.9||the bias momentum. ||
||wc=0 || the L2 weight decay (applied to the weights but not the biases).||

Given these values, the update rule for the weights is:
{{{
weight_inc[i] := momW * weight_inc[i-1] - wc * epsW * weights[i-1] + epsW * weight_grads[i]
weights[i] := weights[i-1] + weight_inc[i]
}}}

where `weight_grads[i]` is the average gradient over minibatch `i`. The update rule for biases is the same except that there is no bias weight decay.

==Logistic regression cost layer==

Cost layers, such as the *logprob* layer defined above, take one parameter:
{{{
[logprob]
coeff=1
}}}

... a scalar coefficient of the objective function. This provides an easy way to tweak the "global" learning rate of the network. Note, however, that tweaking this parameter is not equivalent to tweaking the *epsW* parameter of every layer in the net if you use weight decay. That's because tweaking *coeff* will leave the effective weight decay coefficient (`epsW*wc`) unchanged. But tweaking *epsW* will of course cause a change in `epsW*wc`.

= Layer definition file  - slightly more advanced features =
== Block sparse convolution layer ==

This is a convolutional layer in which each filter only looks at _some_ of the input channels.

{{{
[conv32-2]
type=conv
inputs=cnorm1
groups=4
channels=32
filters=32
padding=2
stride=1
filterSize=5
neuron=relu
initW=0.0001
partialSum=1
sharedBiases=false
}}}

The primary novelty here is the *groups=4* parameter. Together with the *filters=32* parameter, they state that this convolutional layer is to have 4 groups of 32 filters. Each filter will connect to (i.e. sum over) 8 input channels (a quarter of the total number of input channels).

The following diagram depicts the kind of operation that this layer will perform (note however that the diagram depicts two filters per group rather than 32).

http://cuda-convnet.googlecode.com/svn/wiki/images/conv-diagram.png

*Block sparsity may be used in convolutional layers as well as in locally-connected, unshared layers. Simply replace type=conv with type=local to do this in locally-connected, unshared layers.*

===Performance notes===
  * Block sparse convolutional layers are just as efficient computationally as non-sparse convolutional layers.
  * The computation will be most efficient when `channels / groups` is divisible by 16 and `filters` is divisible by 32.

== Random sparse convolution layer ==

A random sparse convolution layer is similar to the block sparse convolution layer described above, but here each group of filters connects to a _random_ subset of channels in the layer below.

Here's how this kind of layer is specified:

{{{
[conv32-3]
type=conv
inputs=conv32-2
groups=2
channels=128
filters=32
padding=2
stride=2
filterSize=5
neuron=relu
initW=0.0001
partialSum=1
randSparse=true
filterChannels=64
}}}

Notice that this layer has 128-channel input, because its input layer, *conv32-2*, has 128 (4 groups of 32) filters. This layer will produce 64-channel output, since it has 2 groups of 32 filters.

There are two new parameters here:

|| *Parameter* || *Default value* || *Meaning* ||
||`randSparse=true` ||`false`|| indicates that random sparsity should be used in this layer. When this parameter is true, the `groups` parameter must be greater than 1 (otherwise the connectivity will not be sparse). ||
||`filterChannels=64` ||`channels/groups`||an optional parameter indicating how many input channels each filter should look at. The quantity `filterChannels*groups` must be a multiple of `channels`.||

*Random sparsity may be used in convolutional layers as well as in locally-connected unshared layers.* For locally-connected unshared layers, each group of *filters* filters at _all locations_ will connect to the same random subset of input channels. So in all there will still be *groups* random subsets, just like in a convolutional layer.

===Performance notes===
  * Random sparse convolutional layers are only about 1% less efficient computationally than block sparse convolutional layers, which themselves are as efficient as non-sparse convolutional layers. So there's no performance reason to avoid them. 
  * The computation will be most efficient when `filterChannels` is divisible by 16 and `filters` is divisible by 32.

==Layers with multiple inputs==

All of the layers with weights (convolutional, locally-connected, and fully-connected) can take multiple layers as input. When there are multiple input layers, a separate set of weights (filters, if you like) is used for each input. Each input/weight pair produces some output, and the final output of the layer is the summation of all those outputs. This implies that all input/weight pairs must produce output of equivalent dimensions.

For example, you can write a convolutional layer with two input layers like this: 
{{{
[conv5b]
type=conv
inputs=conv4b,conv5
filters=64,64
padding=2,2
stride=1,1
filterSize=5,5
channels=64,64
initW=0.01,0.01
initB=1
partialSum=13
groups=1,1
sharedBiases=true
}}}

This convolutional layer takes the two layers *conv4b* and *conv5* as input. Since there are two sets of weights in this layer, you must specify two values for most of the parameters which define the operation that those weights perform on their corresponding input. Some of the parameters remain unary, for example the *initB* parameter, since there is still only one bias vector for all the outputs.

As mentioned above, you must choose sets of parameters that lead to convolutions which produce equivalently-sized output vectors.

A locally-connected, unshared layer with multiple inputs can be specified in exactly the same way, replacing *type=conv* with *type=local*.

A fully-connected layer with multiple inputs can be specified like this:

{{{
[fc1000]
type=fc
outputs=1000
inputs=pool3,probs
initW=0.001,0.1
}}}

In a fully-connected layer, you don't have to do anything to make the number of outputs of the two computations equivalent. The number of outputs is defined by the *outputs* parameter.

==Weight sharing between layers==

Layers may share their weights with other layers of the same kind. One of the things this allows you to do is to write recurrent networks. For example, a layer may take as input the output of another layer which uses *the same weights* -- so in effect the layer is taking its own output (in a previous "time step") as input.

All layers with weights (convolutional, locally-connected, and fully-connected) can share weights. To specify that a layer should use the weight matrix from another layer, add the *weightSource* parameter to the layer's definition file.

Here's an example:

{{{
[conv6a]
type=conv
inputs=conv6
...
weightSource=conv6
}}}

This says that the layer *conv6a* will use the same weight matrix as *conv6*. Of course this requires that the weight matrix of *conv6* have the same shape as the weight matrix that *conv6a* would have had without sharing. 

Note also that the momentum and weight decay parameters given for *conv6a* in the layer parameter file will be ignored. Only layers which own their weight matrix apply momentum and weight decay. Additionally, the *initW* parameter will also be ignored, for obvious reasons.

Now suppose *conv6* has multiple weight matrices (as it would if it took multiple inputs). If you want *conv6a* to share its weight matrix with the *third* weight matrix of *conv6*, you can do it like this:

{{{
[conv6a]
type=conv
inputs=conv6
...
weightSource=conv6[2]
}}}

The bracketed index after the layer name selects a weight matrix from *conv6*. Indices start at 0. If an index is not given, 0 is implied.

Now suppose *conv6a* itself takes three inputs. If you want *conv6a* to take its first and second weight matrix from other layers, you can do it like this:

{{{
[conv6a]
type=conv
inputs=conv6,conv5,conv4
...
weightSource=conv6,,conv4[1]
}}}

Hopefully by now it is clear what this means. The first weight matrix will be taken from *conv6`[0]`*, the second weight matrix will be owned by *conv6a* -- not taken from anywhere, and the third weight matrix will be taken from *conv4`[1]`*.

One final example. Suppose you want *conv6a* to take three inputs but apply the same (newly-defined) filters to all three. In other words, you want *conv6a* to share its weights with itself, amongst its three inputs. Then you can write:

{{{
[conv6a]
type=conv
inputs=conv6,conv5,conv4
...
weightSource=,conv6a,conv6a
}}}

This says that the first weight matrix of *conv6a* will be new, but the remaining two will be equivalent to the first.

= Layer parameter file  - slightly more advanced features =
==Weight layers with multiple inputs==
Layers with weights (convolutional, locally-connected, and fully-connected) which also take multiple inputs must specify a learning rate, weight momentum, and weight decay coefficient for each of its weight matrices.

For example, the parameter file definition for the layer *conv5b* (defined above) is this:

{{{
[conv5b]
epsW=0.001,0.001
epsB=2
momW=0.9,0.9
momB=0.9
wc=0.001,0.001
}}}

= Training the net =

See TrainingNet for details about how to actually cause training to happen.