#summary How to train a neural net with this code.

= Training =

Once you've [Compiling compiled] the code, created a [LayerParams layer definition file], and found some [Data data] to train on, you can actually do some training.

Start by typing
{{{
python convnet.py
}}}

You should get the following output:
{{{
convnet.py usage:
    Option                        Description                  Default 
    [--check-grads <0/1>      ] - Check gradients and quit?    [0]     
    [--epochs <int>           ] - Number of epochs             [50000] 
    [-f <string>              ] - Load file                    []      
    [--gpu <int,...>          ] - GPU override                 []      
    [--max-filesize <int>     ] - Maximum save file size (MB)  [0]     
    [--mini <int>             ] - Minibatch size               [128]   
    [--num-gpus <int>         ] - Number of GPUs               [1]     
    [--test-freq <int>        ] - Testing frequency            [50]    
    [--test-one <0/1>         ] - Test on one batch at a time? [1]     
    [--test-only <0/1>        ] - Test and quit?               [0]     
    [--zip-save <0/1>         ] - Compress checkpoints?        [0]     
     --data-path <string>       - Data path                            
     --data-provider <string>   - Data provider                        
     --layer-def <string>       - Layer definition file                
     --layer-params <string>    - Layer parameter file                 
     --save-path <string>       - Save path                            
     --test-range <int[-int]>   - Data batch range: testing            
     --train-range <int[-int]>  - Data batch range: training                     
}}}

You can read about the meanings of these options [Options here].

You see that you need to provide a data path, a save (checkpointing) path, some parameters to do with which chunks to train on and which chunks to test on, and the layer definition files described [LayerParams here].

So let's do an example. The data for this example will be the CIFAR-10 dataset, which you can download [http://www.cs.toronto.edu/~kriz/cifar-10-py-colmajor.tar.gz here]. If you're not familiar with the CIFAR-10, you can read about it [http://www.cs.toronto.edu/~kriz/cifar.html here].

{{{
python convnet.py --data-path=/storage2/tiny/cifar-10-batches-py-colmajor/ --save-path=/storage2/tmp --test-range=6 --train-range=1-5 --layer-def=./layers.cfg --layer-params=./layer-params.cfg --data-provider=cifar --test-freq=13
}}}

This will cause the net to train on data located in `/storage2/tiny/cifar-10-batches-py-colmajor/` and save checkpoints to `/storage2/tmp`. It will use batches 1-5 for training and batch 6 for testing. For every 13 training batches it processes, it will compute the test error once. The testing frequency is also the checkpoint saving frequency (read more about this below).

Once the net is running, it will start producing output:
{{{
Initialized data layer 'data', producing 3072 outputs
Initialized data layer 'labels', producing 1 outputs
Initialized convolutional layer 'conv32', producing 32x32 32-channel output
Initialized fully-connected layer 'fc1024', producing 1024 outputs
Initialized max-pooling layer 'maxpool', producing 15x15 32-channel output
Initialized convolutional layer 'conv32-2', producing 15x15 32-channel output
Initialized fully-connected layer 'fc10', producing 10 outputs
Initialized softmax layer 'probs', producing 10 outputs
Initialized logistic regression cost 'logprob'
}}}

This just tells you the dimensions of all the layers inside your net. Good to know.

Then this:
{{{
Check gradients and quit?   : 0     [DEFAULT]
Compress checkpoints?       : 0     [DEFAULT]
Data batch range: testing   : 6-6   
Data batch range: training  : 1-5   
Data path                   : /storage2/tiny/cifar-10-batches-py-colmajor/ 
Data provider               : cifar 
GPU override                :       [DEFAULT]
Layer definition file       : ./layers.cfg 
Layer parameter file        : ./layer-params.cfg 
Load file                   :       [DEFAULT]
Maximum save file size (MB) : 0     [DEFAULT]
Minibatch size              : 128   [DEFAULT]
Number of GPUs              : 1     [DEFAULT]
Number of epochs            : 50000 [DEFAULT]
Save path                   : /storage2/tmp 
Test and quit?              : 0     [DEFAULT]
Test on one batch at a time?: 1     [DEFAULT]
}}}

... a printout of the parameters with which the net was run.

And finally, it starts training, and produces output like this:
{{{
Saving checkpoints to /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
=========================
1.1... logprob:  1.855810, 0.351600 (2.076 sec)
1.2... logprob:  1.707988, 0.400900 (2.074 sec)
1.3... logprob:  1.596725, 0.430600 (2.075 sec)
1.4... logprob:  1.615512, 0.433200 (2.088 sec)
1.5... logprob:  1.586030, 0.445700 (2.070 sec)
2.1... logprob:  1.476403, 0.484000 (2.068 sec)
2.2... logprob:  1.481897, 0.489100 (2.067 sec)
2.3... logprob:  1.418266, 0.499000 (2.071 sec)
2.4... logprob:  1.451721, 0.496400 (2.073 sec)
2.5... logprob:  1.428627, 0.506000 (2.085 sec)
3.1... logprob:  1.344407, 0.536500 (2.079 sec)
3.2... logprob:  1.354455, 0.534300 (2.079 sec)
3.3... logprob:  1.298950, 0.543500 
======================Test output======================
logprob:  1.469446, 0.483700 
------------------------------------------------------- 
Layer 'conv32' weights: 0.00063366 [7.23739e-06] 
Layer 'fc1024' weights[0]: 0.000800179 [8.9135e-07] 
Layer 'conv32-2' weights: 0.000622746 [1.23764e-06] 
Layer 'fc10' weights[0]: 0.000803252 [8.5456e-07]
Layer 'fc10' weights[1]: 0.0106858 [9.76545e-05] 
-------------------------------------------------------
Saved checkpoint to /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
======================================================= (3.952 sec)
3.4... logprob:  1.328003, 0.540000 (2.077 sec)
3.5... logprob:  1.307329, 0.547200 (2.076 sec)
4.1... logprob:  1.232344, 0.577200 (2.078 sec)
4.2... logprob:  1.237664, 0.575200 (2.077 sec)
}}}

... which tells you the epoch and batch numbers as well as the values of all of the objectives that you're optimizing. The *cost.logreg* objective happens to return two values -- the average (negative) logprob of the data under the model as well as the classification accuracy. So the second number is actually a percentage. You can see that the net here is at roughly 48% classification accuracy on the test set. This net will continue training until the epoch number exceeds that given to the *--epochs* option (default 50000).

Every time the net prints a test output, it also saves a checkpoint to the path printed. The number of checkpoints that it maintains is controlled by the *--max-filesize* parameter. See [Options] for more on this parameter.

Included with the test output are measures of the magnitudes of the net's weights and weight increments (in square brackets). Specifically, the net is printing the average absolute value of each weight matrix and each weight matrix increment. This output is useful for spotting learning rates that are too low (or too high), causing weights to stagnate (or blow up).

If you want to change the learning parameters you specified, you can just kill the net with Ctrl-C (or kill -9, etc.), change the learning parameters in `layer-params.cfg`, and then restart the net like this:

{{{
python convnet.py -f /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
}}}

The net will resume from the last checkpoint. If you want to resume from an earlier checkpoint, pass a specific checkpoint file inside `/storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011` to the *-f* parameter. The checkpoint files are numbered by their epoch and batch numbers.

== Checking the gradients ==

Now that you know how to run the model, you can [CheckingGradients read] about how to verify that the gradients it computes are correct.