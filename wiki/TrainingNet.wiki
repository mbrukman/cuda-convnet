#summary How to train a neural net with this code.

= Training =

Once you've [Compiling compiled] the code, created a [LayerParams layer definition file], and found some [Data data] to train on, you can actually do some training.

Start by typing
{{{
python convnet.py
}}}

You should get the following output:
{{{
convnet.py usage:
    Option                        Description                  Default 
    [--check-grads <0/1>      ] - Check gradients and quit?    [0]     
    [--epochs <int>           ] - Number of epochs             [50000] 
    [-f <string>              ] - Load file                    []      
    [--gpu <int,...>          ] - GPU override                 []      
    [--max-filesize <int>     ] - Maximum save file size (MB)  [0]     
    [--mini <int>             ] - Minibatch size               [128]   
    [--num-gpus <int>         ] - Number of GPUs               [1]     
    [--test-freq <int>        ] - Testing frequency            [50]    
    [--test-one <0/1>         ] - Test on one batch at a time? [1]     
    [--test-only <0/1>        ] - Test and quit?               [0]     
    [--zip-save <0/1>         ] - Compress checkpoints?        [0]     
     --data-path <string>       - Data path                            
     --data-provider <string>   - Data provider                        
     --layer-def <string>       - Layer definition file                
     --layer-params <string>    - Layer parameter file                 
     --save-path <string>       - Save path                            
     --test-range <int[-int]>   - Data batch range: testing            
     --train-range <int[-int]>  - Data batch range: training                     
}}}

You can read about the meanings of these options [Options here].

You see that you need to provide a data path, a save (checkpointing) path, some parameters to do with which chunks to train on and which chunks to test on, and the layer definition files described [LayerParams here].

So let's do an example. The data for this example will be the CIFAR-10 dataset, which you can download [http://www.cs.toronto.edu/~kriz/cifar-10-py-colmajor.tar.gz here]. If you're not familiar with the CIFAR-10, you can read about it [http://www.cs.toronto.edu/~kriz/cifar.html here].

Try this line, replacing the paths with ones that make sense on your system:
{{{
python convnet.py --data-path=/storage2/tiny/cifar-10-batches-py-colmajor/ --save-path=/storage2/tmp --test-range=6 --train-range=1-5 --layer-def=./layers.cfg --layer-params=./layer-params.cfg --data-provider=cifar --test-freq=13
}}}

This will cause the net to train on data located in `/storage2/tiny/cifar-10-batches-py-colmajor/` and save checkpoints to `/storage2/tmp`. It will use batches 1-5 for training and batch 6 for testing. For every 13 training batches it processes, it will compute the test error once. The testing frequency is also the checkpoint saving frequency (read more about this below).

Once the net is running, it will start producing output:
{{{
Initialized data layer 'data', producing 3072 outputs
Initialized data layer 'labels', producing 1 outputs
Initialized convolutional layer 'conv32', producing 32x32 32-channel output
Initialized fully-connected layer 'fc1024', producing 1024 outputs
Initialized max-pooling layer 'maxpool', producing 15x15 32-channel output
Initialized convolutional layer 'conv32-2', producing 15x15 32-channel output
Initialized fully-connected layer 'fc10', producing 10 outputs
Initialized softmax layer 'probs', producing 10 outputs
Initialized logistic regression cost 'logprob'
}}}

This just tells you the dimensions of all the layers inside your net. Since we never _explicitly_ defined how many outputs each convolutional/pooling layer has, this is good to know.

Then this:
{{{
Check gradients and quit?   : 0     [DEFAULT]
Compress checkpoints?       : 0     [DEFAULT]
Data batch range: testing   : 6-6   
Data batch range: training  : 1-5   
Data path                   : /storage2/tiny/cifar-10-batches-py-colmajor/ 
Data provider               : cifar 
GPU override                :       [DEFAULT]
Layer definition file       : ./layers.cfg 
Layer parameter file        : ./layer-params.cfg 
Load file                   :       [DEFAULT]
Maximum save file size (MB) : 0     [DEFAULT]
Minibatch size              : 128   [DEFAULT]
Number of GPUs              : 1     [DEFAULT]
Number of epochs            : 50000 [DEFAULT]
Save path                   : /storage2/tmp 
Test and quit?              : 0     [DEFAULT]
Test on one batch at a time?: 1     [DEFAULT]
}}}

... a printout of the command-line arguments with which the net was run.

And finally, it starts training, and produces output like this:
{{{
Saving checkpoints to /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
=========================
1.1... logprob:  1.858268, 0.646900 (2.068 sec)
1.2... logprob:  1.708081, 0.595500 (2.064 sec)
1.3... logprob:  1.599059, 0.568600 (2.066 sec)
1.4... logprob:  1.616192, 0.564000 (2.077 sec)
1.5... logprob:  1.588626, 0.554000 (2.064 sec)
2.1... logprob:  1.475589, 0.513100 (2.066 sec)
2.2... logprob:  1.487473, 0.519700 (2.061 sec)
2.3... logprob:  1.421058, 0.501400 (2.069 sec)
2.4... logprob:  1.455423, 0.504700 (2.068 sec)
2.5... logprob:  1.426358, 0.494900 (2.069 sec)
3.1... logprob:  1.343813, 0.467100 (2.069 sec)
3.2... logprob:  1.354268, 0.469400 (2.070 sec)
3.3... logprob:  1.297426, 0.455300 
======================Test output======================
logprob:  1.461566, 0.509800 
------------------------------------------------------- 
Layer 'conv32' weights: 0.000839411 [3.26064e-05] 
Layer 'fc1024' weights[0]: 0.0008 [9.73049e-07] 
Layer 'conv32-2' weights: 0.000701983 [1.1155e-06] 
Layer 'fc10' weights[0]: 0.000806908 [1.17704e-06]
Layer 'fc10' weights[1]: 0.0107289 [9.96676e-05] 
-------------------------------------------------------
Saved checkpoint to /storage2/tmp/ConvNet__Sat_Jul__2_14_16_01_2011
======================================================= (3.992 sec)
3.4... logprob:  1.330210, 0.462500 (2.071 sec)
3.5... logprob:  1.302977, 0.451500 (2.070 sec)
4.1... logprob:  1.233434, 0.431100 (2.070 sec)
4.2... logprob:  1.239093, 0.425300 (2.070 sec)
}}}

... which tells you the epoch and batch numbers as well as the values of all of the objectives that you're optimizing. The *cost.logreg* objective happens to return two values -- the average (negative) logprob of the data under the model as well as the classification error rate. So the second number is actually a percentage. You can see that the net here is at roughly 51% classification error on the test set. This net will continue training until the epoch number exceeds that given to the *--epochs* [Options argument] (default 50000).

Every time the net prints a test output, it also saves a checkpoint to the path printed. The number of checkpoints that it maintains is controlled by the *--max-filesize* parameter. See [Options] for more on this parameter.

Included with the test output are measures of the magnitudes of the net's weights and weight increments (in square brackets). Specifically, the net is printing the average absolute value of each weight matrix and each weight matrix increment. This output is useful for spotting learning rates that are too low (or too high), causing weights to stagnate (or blow up).

If you want to change the learning parameters you specified, you can just kill the net with Ctrl-C (or kill -9, etc.), change the learning parameters in `layer-params.cfg`, and then restart the net like this:

{{{
python convnet.py -f /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
}}}

The net will resume from the last checkpoint. If you want to resume from an earlier checkpoint, pass a specific checkpoint file inside `/storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011` to the *-f* parameter. The checkpoint files are numbered by their epoch and batch numbers.

== Checking the gradients ==

Now that you know how to run the model, you can [CheckingGradients read] about how to verify that the gradients it computes are correct.