#summary How to train a neural net with this code.

= Training =

Once you've [Compiling compiled] the code, created a [LayerParams layer definition file], and found some [Data data] to train on, you can actually do some training.

Start by typing
{{{
python convnet.py
}}}

You should get the following output:
{{{
convnet.py usage:
    Option                        Description                  Default 
    [--check-grads <0/1>      ] - Check gradients and quit?             [0]     
    [--crop-border <int>      ] - Cropped DP: crop border size          [4]     
    [--epochs <int>           ] - Number of epochs                      [50000] 
    [-f <string>              ] - Load file                             []      
    [--gpu <int,...>          ] - GPU override                          []      
    [--logreg-name <string>   ] - Cropped DP: logreg layer name         []      
    [--max-filesize <int>     ] - Maximum save file size (MB)           [0]     
    [--mini <int>             ] - Minibatch size                        [128]   
    [--multiview-test <0/1>   ] - Cropped DP: test on multiple patches? [0]     
    [--num-gpus <int>         ] - Number of GPUs                        [1]     
    [--test-freq <int>        ] - Testing frequency                     [50]    
    [--test-one <0/1>         ] - Test on one batch at a time?          [1]     
    [--test-only <0/1>        ] - Test and quit?                        [0]     
    [--zip-save <0/1>         ] - Compress checkpoints?                 [0]     
     --data-path <string>       - Data path                                     
     --data-provider <string>   - Data provider                                 
     --layer-def <string>       - Layer definition file                         
     --layer-params <string>    - Layer parameter file                          
     --save-path <string>       - Save path                                     
     --test-range <int[-int]>   - Data batch range: testing                     
     --train-range <int[-int]>  - Data batch range: training                   
}}}

You can read about the meanings of these options [Options here].

You see that you need to provide a data path, a save (checkpointing) path, some parameters to do with which chunks to train on and which chunks to test on, and the layer definition files described [LayerParams here].

So let's do an example. The data for this example will be the CIFAR-10 dataset, which you can download [http://www.cs.toronto.edu/~kriz/cifar-10-py-colmajor.tar.gz here]. If you're not familiar with the CIFAR-10, you can read about it [http://www.cs.toronto.edu/~kriz/cifar.html here].

Try this line, replacing the paths with ones that make sense on your system:
{{{
python convnet.py --data-path=/storage2/tiny/cifar-10-batches-py-colmajor/ --save-path=/storage2/tmp --test-range=6 --train-range=1-5 --layer-def=./example-layers/layers-example.cfg --layer-params=./example-layers/layer-params-example.cfg --data-provider=cifar --test-freq=13
}}}

This will cause the net to train on data located in `/storage2/tiny/cifar-10-batches-py-colmajor/` and save checkpoints to `/storage2/tmp`. It will use batches 1-5 for training and batch 6 for testing. For every 13 training batches it processes, it will compute the test error once. The testing frequency is also the checkpoint saving frequency (read more about this below).

Once the net is running, it will start producing output:
{{{
Initialized data layer 'data', producing 3072 outputs
Initialized data layer 'labels', producing 1 outputs
Initialized convolutional layer 'conv32', producing 32x32 32-channel output
Initialized fully-connected layer 'fc1024', producing 1024 outputs
Initialized max-pooling layer 'maxpool', producing 15x15 32-channel output
Initialized response-normalization layer 'cnorm1', producing 15x15 32-channel output
Initialized convolutional layer 'conv32-2', producing 15x15 32-channel output
Initialized fully-connected layer 'fc10', producing 10 outputs
Initialized softmax layer 'probs', producing 10 outputs
Initialized logistic regression cost 'logprob'
}}}

This just tells you the dimensions of all the layers inside your net. Since we never _explicitly_ defined how many outputs each convolutional/pooling layer has, this is good to know.

Then this:
{{{
Check gradients and quit?            : 0   [DEFAULT]
Compress checkpoints?                : 0   [DEFAULT]
Cropped DP: crop border size         : 4   [DEFAULT]
Cropped DP: logreg layer name        :     [DEFAULT]
Cropped DP: test on multiple patches?: 0   [DEFAULT]
Data batch range: testing            : 6-6 
Data batch range: training           : 1-5 
Data path                            : /storage2/tiny/cifar-10-batches-py-colmajor/ 
Data provider                        : cifar 
GPU override                         :     [DEFAULT]
Layer definition file                : ./example-layers/layers-example.cfg 
Layer parameter file                 : ./example-layers/layer-params-example.cfg 
Load file                            :     [DEFAULT]
Maximum save file size (MB)          : 0   [DEFAULT]
Minibatch size                       : 128 [DEFAULT]
Number of GPUs                       : 1   [DEFAULT]
Number of epochs                     : 300 
Save path                            : /storage2/tmp 
Test and quit?                       : 0   [DEFAULT]
Test on one batch at a time?         : 1   [DEFAULT]
Testing frequency                    : 13  
}}}

... a printout of the command-line arguments with which the net was run.

And finally, it starts training, and produces output like this:
{{{
Saving checkpoints to /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
=========================
1.1... logprob:  1.861931, 0.652700 (2.131 sec)
1.2... logprob:  1.709634, 0.597900 (2.111 sec)
1.3... logprob:  1.604382, 0.567600 (2.119 sec)
1.4... logprob:  1.613155, 0.565700 (2.118 sec)
1.5... logprob:  1.586591, 0.553700 (2.111 sec)
2.1... logprob:  1.477137, 0.515700 (2.108 sec)
2.2... logprob:  1.488417, 0.516400 (2.108 sec)
2.3... logprob:  1.426488, 0.500800 (2.119 sec)
2.4... logprob:  1.450616, 0.503900 (2.109 sec)
2.5... logprob:  1.426924, 0.496600 (2.118 sec)
3.1... logprob:  1.347166, 0.467000 (2.117 sec)
3.2... logprob:  1.359413, 0.467600 (2.132 sec)
3.3... logprob:  1.304500, 0.453600
======================Test output======================
logprob:  1.462587, 0.514000 
------------------------------------------------------- 
Layer 'conv32' weights: 7.525693e-04 [8.000028e-06] 
Layer 'fc1024' weights[0]: 8.003611e-04 [8.956434e-07] 
Layer 'conv32-2' weights: 6.535836e-04 [6.687451e-07] 
Layer 'fc10' weights[0]: 8.045786e-04 [7.745636e-07]
Layer 'fc10' weights[1]: 1.067317e-02 [1.005070e-04] 
-------------------------------------------------------
Saved checkpoint to /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
======================================================= (3.992 sec)
3.4... logprob:  1.331711, 0.462800 (2.115 sec)
3.5... logprob:  1.326100, 0.459300 (2.111 sec)
4.1... logprob:  1.260406, 0.437500 (2.108 sec)
4.2... logprob:  1.275028, 0.435800 (2.109 sec)
}}}

... which tells you the epoch and batch numbers as well as the values of all of the objectives that you're optimizing. The *cost.logreg* objective happens to return two values -- the average negative log probability of the data under the model (that's the first number) as well as the classification error rate (the second number). You can see that the net here is at 51.4% classification error on the test set. This net will continue training until the epoch number exceeds that given to the *--epochs* [Options argument] (default 50000).

Every time the net prints a test output, it also saves a checkpoint to the path printed. The number of checkpoints that it maintains is controlled by the *--max-filesize* parameter. See [Options] for more on this parameter.

Included with the test output are measures of the magnitudes of the net's weights and weight increments (in square brackets). Specifically, the net is printing the average absolute value of each weight matrix and each weight matrix increment. This output is useful for spotting learning rates that are too low (or too high), causing weights to stagnate (or blow up).

If you want to change the learning parameters you specified, you can just kill the net with Ctrl-C (or kill -9, etc.), change the learning parameters in `layer-params.cfg`, and then restart the net like this:

{{{
python convnet.py -f /storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011
}}}

The net will resume from the last checkpoint. If you want to resume from an earlier checkpoint, pass a specific checkpoint file inside `/storage2/tmp/ConvNet__Wed_Jun_29_22_19_39_2011` to the *-f* parameter. The checkpoint files are numbered by their epoch and batch numbers.

== Training on image translations == 

Most image classification results can be improved by training on slight translations of the original images. This is an easy way to multiply the amount of training data available, and hence combat over-fitting.

This code ships with a data provider that's capable of training on translations of the CIFAR-10, and it isn't hard to adapt it to other datasets. Given the CIFAR-10, though, here's how you can train on random 24x24 patches of the original 32x32 images:

{{{
python convnet.py --data-path=/storage2/tiny/cifar-10-batches-py-colmajor/ --save-path=/storage2/tmp --test-range=6 --train-range=1-5 --layer-def=./example-layers/layers-example.cfg --layer-params=./example-layers/layer-params-example.cfg --data-provider=cifar-cropped --crop-border=4 --test-freq=13
}}}

This will cause the net to train on random 24x24 patches and test on the center 24x24 patch.

Notice that the only difference between this run line and the one above is that the data provider has been changed to *cifar-cropped* and we've also added a *--crop-border=4* argument which indicates the amount of cropping.

After training is done, you can kill the net and re-run it with the *--test-only=1* and *--multiview-test=1* arguments to compute the test error as averaged over 9 regions of the original images.

== Checking the gradients ==

Now that you know how to run the model, you can [CheckingGradients read] about how to verify that the gradients it computes are correct.